
\section{Dataset}

O conjunto de dados utilizado foi obtido a partir da plataforma Kaggle, no repositório
\textit{Network Intrusion Detection}, disponibilizado publicamente por
Sampada Bhosale~\cite{dataset_kaggle}.

O dataset foi construído a partir da simulação de um ambiente de rede militar típico da
Força Aérea dos Estados Unidos, desenvolvido para representar diferentes tipos de tráfego
de rede, incluindo tanto comunicações legítimas quanto diversos tipos de ataques.
Cada instância do conjunto de dados representa uma conexão de rede, contendo atributos
como a quantidade de bytes transmitidos pela origem, a duração da conexão e outros
parâmetros, conforme descrito na tabela apresentada no Apêndice.

O conjunto de dados contém aproximadamente 25 mil instâncias, descritas por 42
atributos. Após a simulação supervisionada do ambiente, as conexões foram rotuladas como
tráfego normal ou intrusão, caracterizando um problema de classificação binária, no qual
a variável alvo assume os valores \textit{Normal} ou \textit{Anomalia}. A descrição
detalhada dos atributos utilizados é apresentada no Apêndice deste trabalho.
\section{Pré-processamento}
\subsection{Tratamento de Dados}
Devido às diferenças entre os modelos avaliados, as etapas de pré-processamento
foram adaptadas aos requisitos específicos de cada classificador, mantendo a
comparabilidade dos resultados. Inicialmente, o conjunto de dados foi separado
em atributos preditores e rótulos, seguido da divisão em conjuntos de treinamento
e teste por meio de amostragem estratificada.

As variáveis categóricas foram codificadas por meio de \textit{One-Hot Encoding}
ou \textit{Label Encoding}, conforme o modelo utilizado, enquanto as variáveis
numéricas foram padronizadas quando necessário. Além disso, foram conduzidos
experimentos com conjuntos de dados reduzidos, nos quais atributos relevantes
foram removidos de forma controlada, com o objetivo de avaliar a robustez dos
modelos frente à ausência parcial de informações.
A codificação das variáveis categóricas por meio de \textit{One-Hot Encoding} e \textit{Label Encoding} 
foi necessária para permitir que os algoritmos de aprendizagem de máquina processem atributos originalmente representados em formato textual. 
O \textit{One-Hot Encoding} transforma categorias nominais em vetores binários, evitando a introdução de relações ordinais artificiais entre valores distintos, 
sendo especialmente indicado para modelos sensíveis à representação dos dados~\cite{hastie_elements_statistical_learning}. Por outro lado, o \textit{Label Encoding} 
atribui valores inteiros às categorias, apresentando menor custo computacional e sendo apropriado para algoritmos baseados em árvores de decisão, 
nos quais a relação ordinal não compromete o processo de aprendizagem~\cite{geron_hands_on_ml}. 

Adicionalmente, a padronização das variáveis numéricas foi realizada por meio da normalização baseada na média e no desvio padrão, 
garantindo que atributos com escalas distintas contribuíssem de forma equilibrada para o processo de treinamento dos modelos~\cite{bishop_pattern_recognition}. 
A divisão do conjunto de dados em subconjuntos de treinamento e teste foi conduzida utilizando amostragem estratificada, assegurando a preservação da proporção original entre 
tráfego normal e anomalias. Para essa etapa, adotou-se uma proporção de 70\% dos dados para treinamento e 30\% para teste, o que é especialmente relevante em problemas de 
classificação com classes desbalanceadas, pois contribui para uma avaliação mais representativa do desempenho dos modelos~\cite{kohavi_cross_validation}.


\subsection{Seleção e remoção de Atributos}
A aplicação da Análise de Componentes Principais (PCA) como etapa de pré-processamento
tem se mostrado eficaz para a redução da dimensionalidade dos dados sem perda
significativa de informação. Estudos indicam que o PCA é capaz de preservar cerca de
99\% da variância original mesmo com reduções superiores a 50\% no número de atributos,
conforme observado por Santos e Miani~\cite{santos_reducao_dimensao_intrusao}. Nesse
sentido, técnicas de redução e seleção de atributos são amplamente empregadas na
literatura como forma de mitigar redundâncias e melhorar a generalização dos modelos.

É importante destacar a diferença conceitual entre técnicas de redução de dimensionalidade, 
como o PCA, e a remoção manual de atributos adotada neste estudo. Enquanto o PCA realiza 
uma transformação linear dos dados, projetando-os em um novo espaço de menor dimensão 
por meio da combinação dos atributos originais, a abordagem empregada neste trabalho 
consistiu na exclusão direta de variáveis específicas do conjunto de dados. Essa escolha 
permitiu preservar a interpretabilidade das \textit{features} remanescentes, além de 
possibilitar a análise individual do impacto da ausência de cada atributo no desempenho 
dos classificadores. Ademais, essa estratégia representa de forma mais fiel cenários 
práticos, nos quais nem sempre todas as informações ou sensores estão disponíveis, 
contribuindo para a avaliação da robustez dos modelos frente à perda parcial de dados.





