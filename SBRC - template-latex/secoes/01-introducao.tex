\section{Introdução}

O crescimento contínuo da demanda por serviços digitais, tanto no cotidiano quanto em ambientes corporativos, tem impulsionado a ampla utilização de redes de computadores, incluindo data centers e servidores privados virtuais (VPS). Como consequência, observou-se um aumento significativo no número e na complexidade das ameaças, de acordo com o Relatório de Índice de Cibersegurança da ENISA (2024)\cite{enisa2025}, embora haja um esforço para fortalecer a resiliência cibernética, a capacidade do setor privado em detectar e analisar incidentes ainda enfrenta desafios de maturidade técnica. Essa evolução torna a detecção de intrusões um dos principais desafios da área de segurança da informação. Ataques cada vez mais sofisticados exigem mecanismos capazes de identificar comportamentos anômalos de forma precisa e eficiente, mesmo em cenários caracterizados por grandes volumes de dados e alta dimensionalidade.

Nesse contexto, técnicas de aprendizado de máquina têm sido amplamente empregadas em sistemas de detecção de intrusões\cite{buczak2015survey}\cite{zamani2013machine}\cite{he2023adversarial}, devido à sua capacidade de aprender padrões complexos a partir do tráfego de rede e de se adaptar a diferentes tipos de ataques. Historicamente, a eficácia desses sistemas depende da qualidade das bases de dados utilizadas para o treinamento. Conforme analisado previamente no artigo "A detailed analysis of the KDD CUP 99 data set" \cite{tavallaee2009detailed}, cojunto de dados podem apresentar deficiências como a redundância de registros que foi avaliado no conjunto do KDD CUP 99, dataset criado para uma competição da área de mineração de dados, baseando-se em tráfego de rede simulado para distinguir entre conexões normais e invasões. O que pode causar um viés nos classificadores em direção a padrões frequentes, resultando em taxas de acerto inflacionadas que não refletem o desempenho real em redes modernas.

Diversos estudos na literatura investigam estratégias automáticas de redução de dimensionalidade e seleção de atributos, como a Análise de Componentes Principais (PCA), com o objetivo de melhorar a eficiência computacional e a capacidade de generalização dos modelos\cite{jia2022feature}\cite{mladenic2005feature}\cite{zaheer2025evaluating}. Apesar disso, ainda há uma lacuna na compreensão do impacto individual das features mais relevantes no desempenho de diferentes classificadores, especialmente quando tais atributos deixam de estar disponíveis. Dessa forma, este trabalho tem como objetivo analisar a influência da seleção e da remoção de atributos no desempenho de diferentes modelos de aprendizado de máquina aplicados à detecção de anomalias em redes de computadores. Avaliando modelos supervisionados e não supervisionados, incluindo Random Forest, K-Nearest Neighbors (KNN), Multilayer Perceptron (MLP) e Isolation Forest. Os experimentos buscam, assim, fornecer uma análise comparativa que contribua para a compreensão da robustez e das limitações de cada abordagem em cenários realistas de detecção de intrusões.